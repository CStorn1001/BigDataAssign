{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the mongo client \n",
    "client = MongoClient = MongoClient(port = 27017)\n",
    "#select the mongoDB database\n",
    "db = client[\"COMP3210-ASSIGN1\"]\n",
    "#select the mongodb tweets dataset\n",
    "tweets = db[\"10000 tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'COMP3210-ASSIGN1'), '10000 tweets')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pymongo.collection.Collection"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweets)\n",
    "type(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function remove all stop words\n",
    "def remove_stopwords(words):\n",
    "    result =[]\n",
    "    for w in words:\n",
    "        if w not in stopwords.words(\"english\"):\n",
    "            result.append(w)\n",
    "    return result\n",
    "#function to develop stem words through porter stemmer\n",
    "def stem_words(words):\n",
    "    ps = PorterStemmer()\n",
    "    result =[ps.stem(w) for w in words]\n",
    "    return result\n",
    "\n",
    "#function to find keywords within text uses both remove stop words and stem words\n",
    "def find_keywords_in_text(words):\n",
    "    text = re.sub(r\"[^\\w]\", \" \", words)\n",
    "    words = word_tokenize(text)\n",
    "    words=remove_stopwords(words)\n",
    "    words=stem_words(words)\n",
    "    keywords=nltk.pos_tag(words)\n",
    "    return keywords\n",
    "\n",
    "#converting keywords into a common speator to text\n",
    "def keywords_to_csv(keywords):\n",
    "    csv:str= \"\"\n",
    "    for k in keywords:\n",
    "        column: str = k[0]+\",\"+k[1]\n",
    "        csv = csv + column + \"\\n\"\n",
    "    return csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loops through each tweet, and extracts keywords\n",
    "# updates the database with the keywords in CSV format\n",
    "# 1 for using and 0 for not using\n",
    "for t in tweets.find({},{\"id\":1, \"body\":1, \"_id\":0}):\n",
    "    keywords = find_keywords_in_text(t[\"body\"])\n",
    "    csv=keywords_to_csv(keywords)\n",
    "    tweets.update_many({\"id\":t[\"id\"]}, {\"$set\":{\"keyword\":csv}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove URLS from https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python\n",
    "#Remove Emojis https://www.educative.io/edpresso/how-to-remove-emoji-from-the-text-in-python\n",
    "# in command prompt pip install clean-text\n",
    "# must make all words lowercase so we do not make the problem of eg best and BEST not being the same\n",
    "# This url helped to complete the pre-procesing steps for twitter : https://www3.tuhh.de/sts/hoou/data-quality-explored/3-2-simple-transf.html\n",
    "from unidecode import unidecode # helps to decode more efficently with minimal bias\n",
    "\n",
    "def remove_urls(txt):\n",
    "    # txt = re.sub(r\"http\\S+\", \"\", txt) # flags=re.MULTILINE)\n",
    "    txt = re.sub(r'https?://[^ ]+', '', txt)\n",
    "    return(txt)\n",
    "def remove_user(txt):\n",
    "    txt = re.sub(r'@[^ ]+', '', txt)\n",
    "    return txt\n",
    "def remove_hashtag(txt):\n",
    "    txt = re.sub(r'#', '', txt)\n",
    "    return txt\n",
    "#character normalisation\n",
    "def char_norm(txt):\n",
    "    txt = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', txt)\n",
    "    return txt\n",
    "#removing punctuation, special characters and numbers\n",
    "def remove_pun_sc_nums(txt):\n",
    "    txt = re.sub(r' 0 ', 'zero', txt)\n",
    "    txt = re.sub(r'[^A-Za-z ]', '', txt)\n",
    "    return txt\n",
    "#Remove Emojis https://www.educative.io/edpresso/how-to-remove-emoji-from-the-text-in-python\n",
    "# in command prompt pip install clean-text\n",
    "from cleantext import clean\n",
    "def clean_data(txt):\n",
    "    return remove_pun_sc_nums(char_norm(remove_hashtag(remove_user(remove_urls(clean(txt.lower(), no_emoji=True))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add all posts within a string variable so then I can use my clean_data function, word tokenize and remove stop words\n",
    "body_text = ''\n",
    "for t in tweets.find():\n",
    "    body_text+=t[\"body\"]\n",
    "tok_body_text = remove_stopwords(word_tokenize(clean_data(body_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out into a text file to use for my task 2 mapreduce function\n",
    "with open('Task2_body_cleaned.txt', 'w', encoding='utf-8') as f:\n",
    "    for t in tok_body_text:\n",
    "        f.write(t)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input file for task 3\n",
    "# aus_cities  = ('Sydney' or 'Brisbane' or 'Mebourne'or 'Perth'or 'Adelaide'or 'Hobart'or 'Canberra' or 'Darwin')\n",
    "# Due to getting not all cities from australia when filtering twitterzone and display name. I create a list which if contains the city will print into text file\n",
    "aus_cities  = ['Sydney','Brisbane','Melbourne','Perth','Adelaide','Hobart','Canberra','Darwin']\n",
    "with open('Task3_cities.txt', 'w', encoding='utf-8') as f:\n",
    "    for t in tweets.find({\"actor.location.displayName\": {'$regex' : 'Australia', \"$options\" : \"i\"}}):\n",
    "        if(t[\"actor\"][\"twitterTimeZone\"] != None and t[\"actor\"][\"twitterTimeZone\"] in aus_cities):\n",
    "            f.write(t[\"actor\"][\"twitterTimeZone\"])\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Merge sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab ids and splits the extra stuff such as the twitter.com part to only get the ids.\n",
    "#used for both merge and bucket sort\n",
    "with open('task4_input.txt', 'w', encoding='utf-8') as f:\n",
    "    for t in tweets.find():\n",
    "        f.write(t[\"object\"][\"id\"].split(':')[2])\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocesing is found within Task 4- Merge section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2', '7', '3', '4', '5', '6'}\n"
     ]
    }
   ],
   "source": [
    "#determining all twitter id first digit numbers\n",
    "leng = [] \n",
    "for t in tweets.find():\n",
    "    leng.append(t[\"object\"][\"id\"].split(':')[2][0])\n",
    "    \n",
    "leng = set(leng)\n",
    "print(leng)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Two': 1, 'Three': 2, 'Four': 6, 'Five': 11, 'Six': 65, 'Seven': 9915, 'Eight': 0}\n"
     ]
    }
   ],
   "source": [
    "##Checking the ratio of digits with particular first digit numbers within the twitter ids\n",
    "two = 0\n",
    "three = 0\n",
    "four = 0\n",
    "five = 0\n",
    "six = 0\n",
    "seven = 0\n",
    "eight = 0\n",
    "total = 0\n",
    "for t in tweets.find():\n",
    "    if t[\"object\"][\"id\"].split(':')[2][0] == '2':\n",
    "        two+=1\n",
    "    if t[\"object\"][\"id\"].split(':')[2][0] == '3':\n",
    "        three+=1\n",
    "    if t[\"object\"][\"id\"].split(':')[2][0] == '4':\n",
    "        four+=1\n",
    "    if t[\"object\"][\"id\"].split(':')[2][0] == '5':\n",
    "        five+=1\n",
    "    if t[\"object\"][\"id\"].split(':')[2][0] == '6':\n",
    "        six+=1\n",
    "    if t[\"object\"][\"id\"].split(':')[2][0] == '7':\n",
    "        seven+=1\n",
    "    total+=1\n",
    "print({\"Two\" :two, \"Three\" :three, \"Four\" :four, \"Five\" :five, \"Six\" :six, \"Seven\": seven, \"Eight\": eight})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Five': 0.04, 'Six': 0.11, 'Seven': 0.54, 'Eight': 18.75, 'Nine': 34.72, 'Ten': 42.21, 'Eighteen': 3.63}\n"
     ]
    }
   ],
   "source": [
    "# print({\"Five\" :(five/total)*100, \"Six\" :(six/total)*100, \"Seven\": (seven/total)*100, \"Eight\": (eight/total)*100, \"Nine\":(nine/total)*100, \"Ten\":(ten/total)*100, \"Eighteen\": (eighteen/total)*100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#developed a dictionary where keys is twitter ids and values are twitter post text\n",
    "# I did this as I found it an easy approach to using my cleaning data for my post text (which is find necessary)\n",
    "ids_text_dic = {}\n",
    "for i in tweets.find():\n",
    "    ids_text_dic.update({i[\"id\"].split(':')[2] : clean_data(i[\"body\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#did this to determine how many times health occurances in each post, based on results tweets are found mostly once in a document which contains it.\n",
    "count_health = []\n",
    "for i in ids_text_dic.values():\n",
    "    if 'health' in i:\n",
    "        count_health.append(i.count(\"health\"))\n",
    "print(set(count_health)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocessing my dictionary data into a text file where I used this symbol : so I can split both parts into an array within my mapreduce function\n",
    "with open('Task5_ids_body.txt', 'w', encoding='utf-8') as f:\n",
    "    for ids ,body in ids_text_dic.items():\n",
    "        f.write(ids)\n",
    "        f.write(':')\n",
    "        f.write(body)\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9337a79df3e5d869f0c5777e0d2b1209a728c6a52c1d44e22a85de3e5a3843fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
